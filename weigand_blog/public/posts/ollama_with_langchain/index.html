<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>Running Ollama Through LangChain :: Terminal</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content=" Table of Contents Running Ollama Through Docker Interacting With Ollama through LangChain Working Example Problems Running Ollama Through Docker Using the Docker Image on DockerHub, I created a containerized environment to run ollama and natural language machine learning models locally to be used within LangChain. As of right now, we I am only focusing on using CPU to generate tokens, we can work on GPU Acceleration later.
DockerHub Image -&gt; image Ollama Model -&gt; gemma2b More specifically, I chose gemma2b as a lightweight model for fast testing but mostly coherent responses.
" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/ollama_with_langchain/" />





  
  <link rel="stylesheet" href="//localhost:1313/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">


<link rel="stylesheet" href="//localhost:1313/terminal.css">




<link rel="shortcut icon" href="//localhost:1313/favicon.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Running Ollama Through LangChain">
<meta property="og:description" content=" Table of Contents Running Ollama Through Docker Interacting With Ollama through LangChain Working Example Problems Running Ollama Through Docker Using the Docker Image on DockerHub, I created a containerized environment to run ollama and natural language machine learning models locally to be used within LangChain. As of right now, we I am only focusing on using CPU to generate tokens, we can work on GPU Acceleration later.
DockerHub Image -&gt; image Ollama Model -&gt; gemma2b More specifically, I chose gemma2b as a lightweight model for fast testing but mostly coherent responses.
" />
<meta property="og:url" content="//localhost:1313/posts/ollama_with_langchain/" />
<meta property="og:site_name" content="Terminal" />

  <meta property="og:image" content="//localhost:1313/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2025-10-17 00:00:00 &#43;0000 UTC" />












</head>
<body>


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/showcase">Showcase</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="/showcase" >Showcase</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/ollama_with_langchain/">Running Ollama Through LangChain</a>
  </h1>
  <div class="post-meta"><time class="post-date">2025-10-17</time></div>

  
  


  

  <div class="post-content"><div>
        <hr>
<h1 id="table-of-contents">Table of Contents<a href="#table-of-contents" class="hanchor" ariaLabel="Anchor">#</a> </h1>
<ol>
<li><a href="/posts/ollama_with_langchain/#running-ollama-through-docker">Running Ollama Through Docker</a></li>
<li><a href="/posts/ollama_with_langchain/#interacting-with-ollama-through-langchain">Interacting With Ollama through LangChain</a></li>
<li><a href="/posts/ollama_with_langchain/#working-example">Working Example</a></li>
<li><a href="/posts/ollama_with_langchain/#problems">Problems</a></li>
</ol>
<hr>
<h2 id="running-ollama-through-docker">Running Ollama Through Docker<a href="#running-ollama-through-docker" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>Using the Docker Image on DockerHub, I created a containerized environment to run ollama and natural language machine learning models locally to be used within LangChain. As of right now, we I am only focusing on using CPU to generate tokens, we can work on GPU Acceleration later.</p>
<ol>
<li>DockerHub Image -&gt;  <a href="https://hub.docker.com/r/ollama/ollama">image</a></li>
<li>Ollama Model -&gt; <a href="https://ollama.com/library/gemma:2b">gemma2b</a></li>
</ol>
<p>More specifically, I chose gemma2b as a lightweight model for fast testing but mostly coherent responses.</p>
<hr>
<h2 id="interacting-with-ollama-through-langchain">Interacting with Ollama through LangChain<a href="#interacting-with-ollama-through-langchain" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>Example provided by LangChain:</p>
<pre tabindex="0"><code>from langchain_core.prompts import ChatPromptTemplate  
from langchain_ollama.llms import OllamaLLM  
  
template = &#34;&#34;&#34;Question: {question}  
  
Answer: Let&#39;s think step by step.&#34;&#34;&#34;  
  
prompt = ChatPromptTemplate.from_template(template)  
  
model = OllamaLLM(model=&#34;llama3.1&#34;)  
  
chain = prompt | model  
  
chain.invoke({&#34;question&#34;: &#34;What is LangChain?&#34;})
</code></pre><p>My example:</p>
<pre tabindex="0"><code>from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate


def main():
    model = OllamaLLM(model=&#34;gemma2:2b&#34;, base_url=&#34;http://localhost:11434&#34;)

    template = &#34;Explain this like I am 5 years old: {text}&#34;
    prompt = ChatPromptTemplate.from_template(template)

    chain = prompt | model

    explain_item = input(&#34;Enter what you want explained -&gt; &#34;)

    result = chain.invoke({&#34;text&#34;: explain_item})
    print(&#34;Response: &#34;, result)


if __name__ == &#34;__main__&#34;:
    main()
</code></pre><hr>
<h2 id="working-example">Working Example<a href="#working-example" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>This is an instance of it working (may have to open image in new tab):</p>
<p><img src="/posts/ollama_with_langchain/working_langchain.png" alt="working_langchain"></p>
<hr>
<h2 id="problems">Problems<a href="#problems" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>Some of the problems that I encounters was that when running Ollama through Docker, the models installed will be removed after the deletion of the container. A solution to this is to install ollama locally, and install all the models locally and mount the model directory to the docker container to avoid downloads and deletions.
Some of the problems that I encounters was that when running Ollama through Docker, the models installed will be removed after the deletion of the container. A solution to this is to install ollama locally, and install all the models locally and mount the model directory to the docker container to avoid downloads and deletions.</p>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h">Read other posts</span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
      <a href="//localhost:1313/posts/ollama_integration/" class="button inline prev">
        &lt; [<span class="button__text">Ollama Integration</span>]
      </a>
    
    
      ::
    
    
      <a href="//localhost:1313/posts/architecture/" class="button inline next">
         [<span class="button__text">Capstone Architecture Discussion</span>] &gt;
      </a>
    
  </div>
</div>


  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
