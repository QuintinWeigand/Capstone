<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>Whisper Integration for Voice Input :: Terminal</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content=" Table of Contents Overview Whisper Model Setup Audio Processing Pipeline Browser Audio Recording Backend Integration Error Handling Overview Voice input capability was added to the multi-tool assistant using OpenAI&rsquo;s Whisper model for speech-to-text transcription. This allows users to interact with the AI assistant through natural speech, making the interface more accessible and user-friendly.
Whisper Model Setup Model Selection Model: Whisper base model Device: CPU-based processing for compatibility Language: Auto-detection for multilingual support Loading Strategy def load_whisper_model(): global whisper_model if whisper_model is None: try: # Load model with FP32 to avoid FP16 CPU warning whisper_model = whisper.load_model(&#34;base&#34;, device=&#34;cpu&#34;) except Exception as e: print(f&#34;Error loading Whisper model: {e}&#34;) raise e return whisper_model Performance Considerations Lazy loading: Model loaded only when needed Global instance: Single model instance for efficiency CPU optimization: FP32 precision to avoid warnings Audio Processing Pipeline Transcription Process def transcribe_audio(audio_file_path): try: model = load_whisper_model() result = model.transcribe(audio_file_path) # Handle both string and list cases for text text = result[&#34;text&#34;] if isinstance(text, list): text = &#34; &#34;.join(text) elif isinstance(text, str): text = text.strip() return text except Exception as e: print(f&#34;Transcription error: {e}&#34;) return f&#34;Transcription Error: {e}&#34; File Management Temporary files: Use tempfile.NamedTemporaryFile for audio storage Automatic cleanup: Files deleted after processing Format support: Handles WebM audio from browser recording Browser Audio Recording MediaRecorder API Integration async function toggleRecording(event) { if (isRecording) { // Stop recording mediaRecorder.stop(); } else { // Start recording const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: true, noiseSuppression: true, sampleRate: 16000 } }); mediaRecorder = new MediaRecorder(stream, { mimeType: &#39;audio/webm;codecs=opus&#39; }); mediaRecorder.start(); } } Audio Configuration Echo cancellation: Reduces feedback and echo Noise suppression: Improves audio quality Sample rate: 16kHz for optimal Whisper performance Format: WebM with Opus codec for browser compatibility Backend Integration Flask Endpoint @app.route(&#34;/transcribe&#34;, methods=[&#34;POST&#34;]) def transcribe_audio_endpoint(): if &#34;audio&#34; not in request.files: return jsonify({&#34;error&#34;: &#34;No audio file&#34;}), 400 audio_file = request.files[&#34;audio&#34;] with tempfile.NamedTemporaryFile(delete=False, suffix=&#34;.webm&#34;) as tmp: audio_file.save(tmp.name) # Transcribe audio transcribed_text = transcribe_audio(tmp.name) # Process transcribed text through existing pipeline response = process_message(transcribed_text) # Cleanup os.unlink(tmp.name) return jsonify({ &#34;user_input&#34;: transcribed_text, &#34;response&#34;: response }) Integration Flow Audio upload from browser to Flask endpoint Temporary file storage for processing Whisper transcription converts speech to text AI processing through existing message pipeline JSON response returns both transcription and AI response File cleanup removes temporary audio file Error Handling Client-Side Errors Microphone access: Graceful handling of permission denials Recording failures: User-friendly error messages Network issues: Timeout and connection error handling Server-Side Errors File validation: Check for audio file presence Transcription failures: Catch and report Whisper errors Cleanup assurance: Files removed even if processing fails User Feedback function showError(message) { const errorDiv = document.createElement(&#39;div&#39;); errorDiv.style.cssText = &#39;background-color: #f8d7da;&#39; &#43; &#39;color: #721c24;&#39; &#43; &#39;padding: 10px;&#39; &#43; &#39;border-radius: 5px;&#39; &#43; &#39;margin: 10px 0;&#39;; errorDiv.textContent = message; // Auto-remove after 5 seconds setTimeout(function() { if (errorDiv.parentNode) { errorDiv.parentNode.removeChild(errorDiv); } }, 5000); } Conclusion The Whisper integration successfully adds voice input capabilities to the AI assistant, providing a more natural and accessible user interface. The implementation demonstrates:
" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/whisper_integration/" />





  
  <link rel="stylesheet" href="//localhost:1313/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">


<link rel="stylesheet" href="//localhost:1313/terminal.css">




<link rel="shortcut icon" href="//localhost:1313/favicon.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Whisper Integration for Voice Input">
<meta property="og:description" content=" Table of Contents Overview Whisper Model Setup Audio Processing Pipeline Browser Audio Recording Backend Integration Error Handling Overview Voice input capability was added to the multi-tool assistant using OpenAI&rsquo;s Whisper model for speech-to-text transcription. This allows users to interact with the AI assistant through natural speech, making the interface more accessible and user-friendly.
Whisper Model Setup Model Selection Model: Whisper base model Device: CPU-based processing for compatibility Language: Auto-detection for multilingual support Loading Strategy def load_whisper_model(): global whisper_model if whisper_model is None: try: # Load model with FP32 to avoid FP16 CPU warning whisper_model = whisper.load_model(&#34;base&#34;, device=&#34;cpu&#34;) except Exception as e: print(f&#34;Error loading Whisper model: {e}&#34;) raise e return whisper_model Performance Considerations Lazy loading: Model loaded only when needed Global instance: Single model instance for efficiency CPU optimization: FP32 precision to avoid warnings Audio Processing Pipeline Transcription Process def transcribe_audio(audio_file_path): try: model = load_whisper_model() result = model.transcribe(audio_file_path) # Handle both string and list cases for text text = result[&#34;text&#34;] if isinstance(text, list): text = &#34; &#34;.join(text) elif isinstance(text, str): text = text.strip() return text except Exception as e: print(f&#34;Transcription error: {e}&#34;) return f&#34;Transcription Error: {e}&#34; File Management Temporary files: Use tempfile.NamedTemporaryFile for audio storage Automatic cleanup: Files deleted after processing Format support: Handles WebM audio from browser recording Browser Audio Recording MediaRecorder API Integration async function toggleRecording(event) { if (isRecording) { // Stop recording mediaRecorder.stop(); } else { // Start recording const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: true, noiseSuppression: true, sampleRate: 16000 } }); mediaRecorder = new MediaRecorder(stream, { mimeType: &#39;audio/webm;codecs=opus&#39; }); mediaRecorder.start(); } } Audio Configuration Echo cancellation: Reduces feedback and echo Noise suppression: Improves audio quality Sample rate: 16kHz for optimal Whisper performance Format: WebM with Opus codec for browser compatibility Backend Integration Flask Endpoint @app.route(&#34;/transcribe&#34;, methods=[&#34;POST&#34;]) def transcribe_audio_endpoint(): if &#34;audio&#34; not in request.files: return jsonify({&#34;error&#34;: &#34;No audio file&#34;}), 400 audio_file = request.files[&#34;audio&#34;] with tempfile.NamedTemporaryFile(delete=False, suffix=&#34;.webm&#34;) as tmp: audio_file.save(tmp.name) # Transcribe audio transcribed_text = transcribe_audio(tmp.name) # Process transcribed text through existing pipeline response = process_message(transcribed_text) # Cleanup os.unlink(tmp.name) return jsonify({ &#34;user_input&#34;: transcribed_text, &#34;response&#34;: response }) Integration Flow Audio upload from browser to Flask endpoint Temporary file storage for processing Whisper transcription converts speech to text AI processing through existing message pipeline JSON response returns both transcription and AI response File cleanup removes temporary audio file Error Handling Client-Side Errors Microphone access: Graceful handling of permission denials Recording failures: User-friendly error messages Network issues: Timeout and connection error handling Server-Side Errors File validation: Check for audio file presence Transcription failures: Catch and report Whisper errors Cleanup assurance: Files removed even if processing fails User Feedback function showError(message) { const errorDiv = document.createElement(&#39;div&#39;); errorDiv.style.cssText = &#39;background-color: #f8d7da;&#39; &#43; &#39;color: #721c24;&#39; &#43; &#39;padding: 10px;&#39; &#43; &#39;border-radius: 5px;&#39; &#43; &#39;margin: 10px 0;&#39;; errorDiv.textContent = message; // Auto-remove after 5 seconds setTimeout(function() { if (errorDiv.parentNode) { errorDiv.parentNode.removeChild(errorDiv); } }, 5000); } Conclusion The Whisper integration successfully adds voice input capabilities to the AI assistant, providing a more natural and accessible user interface. The implementation demonstrates:
" />
<meta property="og:url" content="//localhost:1313/posts/whisper_integration/" />
<meta property="og:site_name" content="Terminal" />

  <meta property="og:image" content="//localhost:1313/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2025-12-14 00:00:00 &#43;0000 UTC" />












</head>
<body>


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/showcase">Showcase</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="/showcase" >Showcase</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/whisper_integration/">Whisper Integration for Voice Input</a>
  </h1>
  <div class="post-meta"><time class="post-date">2025-12-14</time></div>

  
  


  

  <div class="post-content"><div>
        <hr>
<h1 id="table-of-contents">Table of Contents<a href="#table-of-contents" class="hanchor" ariaLabel="Anchor">#</a> </h1>
<ol>
<li><a href="/posts/whisper_integration/#overview">Overview</a></li>
<li><a href="/posts/whisper_integration/#whisper_model_setup">Whisper Model Setup</a></li>
<li><a href="/posts/whisper_integration/#audio_processing_pipeline">Audio Processing Pipeline</a></li>
<li><a href="/posts/whisper_integration/#browser_audio_recording">Browser Audio Recording</a></li>
<li><a href="/posts/whisper_integration/#backend_integration">Backend Integration</a></li>
<li><a href="/posts/whisper_integration/#error_handling">Error Handling</a></li>
</ol>
<hr>
<h2 id="overview">Overview<a href="#overview" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>Voice input capability was added to the multi-tool assistant using OpenAI&rsquo;s Whisper model for speech-to-text transcription. This allows users to interact with the AI assistant through natural speech, making the interface more accessible and user-friendly.</p>
<hr>
<h2 id="whisper-model-setup">Whisper Model Setup<a href="#whisper-model-setup" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="model-selection">Model Selection<a href="#model-selection" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Model</strong>: Whisper <code>base</code> model</li>
<li><strong>Device</strong>: CPU-based processing for compatibility</li>
<li><strong>Language</strong>: Auto-detection for multilingual support</li>
</ul>
<h3 id="loading-strategy">Loading Strategy<a href="#loading-strategy" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">load_whisper_model</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">global</span> <span class="n">whisper_model</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">whisper_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Load model with FP32 to avoid FP16 CPU warning</span>
</span></span><span class="line"><span class="cl">            <span class="n">whisper_model</span> <span class="o">=</span> <span class="n">whisper</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;base&#34;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Error loading Whisper model: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">whisper_model</span>
</span></span></code></pre></div><h3 id="performance-considerations">Performance Considerations<a href="#performance-considerations" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Lazy loading</strong>: Model loaded only when needed</li>
<li><strong>Global instance</strong>: Single model instance for efficiency</li>
<li><strong>CPU optimization</strong>: FP32 precision to avoid warnings</li>
</ul>
<hr>
<h2 id="audio-processing-pipeline">Audio Processing Pipeline<a href="#audio-processing-pipeline" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="transcription-process">Transcription Process<a href="#transcription-process" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transcribe_audio</span><span class="p">(</span><span class="n">audio_file_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">load_whisper_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transcribe</span><span class="p">(</span><span class="n">audio_file_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Handle both string and list cases for text</span>
</span></span><span class="line"><span class="cl">        <span class="n">text</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&#34;text&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">text</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">text</span>
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Transcription error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="sa">f</span><span class="s2">&#34;Transcription Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div><h3 id="file-management">File Management<a href="#file-management" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Temporary files</strong>: Use <code>tempfile.NamedTemporaryFile</code> for audio storage</li>
<li><strong>Automatic cleanup</strong>: Files deleted after processing</li>
<li><strong>Format support</strong>: Handles WebM audio from browser recording</li>
</ul>
<hr>
<h2 id="browser-audio-recording">Browser Audio Recording<a href="#browser-audio-recording" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="mediarecorder-api-integration">MediaRecorder API Integration<a href="#mediarecorder-api-integration" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="line"><span class="cl"><span class="kr">async</span> <span class="kd">function</span> <span class="nx">toggleRecording</span><span class="p">(</span><span class="nx">event</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="nx">isRecording</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Stop recording
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="nx">mediaRecorder</span><span class="p">.</span><span class="nx">stop</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Start recording
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kr">const</span> <span class="nx">stream</span> <span class="o">=</span> <span class="kr">await</span> <span class="nx">navigator</span><span class="p">.</span><span class="nx">mediaDevices</span><span class="p">.</span><span class="nx">getUserMedia</span><span class="p">({</span> 
</span></span><span class="line"><span class="cl">            <span class="nx">audio</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nx">echoCancellation</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nx">noiseSuppression</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nx">sampleRate</span><span class="o">:</span> <span class="mi">16000</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span> 
</span></span><span class="line"><span class="cl">        <span class="p">});</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="nx">mediaRecorder</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">MediaRecorder</span><span class="p">(</span><span class="nx">stream</span><span class="p">,</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nx">mimeType</span><span class="o">:</span> <span class="s1">&#39;audio/webm;codecs=opus&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="p">});</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="nx">mediaRecorder</span><span class="p">.</span><span class="nx">start</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h3 id="audio-configuration">Audio Configuration<a href="#audio-configuration" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Echo cancellation</strong>: Reduces feedback and echo</li>
<li><strong>Noise suppression</strong>: Improves audio quality</li>
<li><strong>Sample rate</strong>: 16kHz for optimal Whisper performance</li>
<li><strong>Format</strong>: WebM with Opus codec for browser compatibility</li>
</ul>
<hr>
<h2 id="backend-integration">Backend Integration<a href="#backend-integration" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="flask-endpoint">Flask Endpoint<a href="#flask-endpoint" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@app.route</span><span class="p">(</span><span class="s2">&#34;/transcribe&#34;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;POST&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transcribe_audio_endpoint</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s2">&#34;audio&#34;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">request</span><span class="o">.</span><span class="n">files</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">jsonify</span><span class="p">({</span><span class="s2">&#34;error&#34;</span><span class="p">:</span> <span class="s2">&#34;No audio file&#34;</span><span class="p">}),</span> <span class="mi">400</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">audio_file</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">files</span><span class="p">[</span><span class="s2">&#34;audio&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s2">&#34;.webm&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmp</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">audio_file</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tmp</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Transcribe audio</span>
</span></span><span class="line"><span class="cl">        <span class="n">transcribed_text</span> <span class="o">=</span> <span class="n">transcribe_audio</span><span class="p">(</span><span class="n">tmp</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Process transcribed text through existing pipeline</span>
</span></span><span class="line"><span class="cl">        <span class="n">response</span> <span class="o">=</span> <span class="n">process_message</span><span class="p">(</span><span class="n">transcribed_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Cleanup</span>
</span></span><span class="line"><span class="cl">        <span class="n">os</span><span class="o">.</span><span class="n">unlink</span><span class="p">(</span><span class="n">tmp</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">jsonify</span><span class="p">({</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;user_input&#34;</span><span class="p">:</span> <span class="n">transcribed_text</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;response&#34;</span><span class="p">:</span> <span class="n">response</span>
</span></span><span class="line"><span class="cl">        <span class="p">})</span>
</span></span></code></pre></div><h3 id="integration-flow">Integration Flow<a href="#integration-flow" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ol>
<li><strong>Audio upload</strong> from browser to Flask endpoint</li>
<li><strong>Temporary file storage</strong> for processing</li>
<li><strong>Whisper transcription</strong> converts speech to text</li>
<li><strong>AI processing</strong> through existing message pipeline</li>
<li><strong>JSON response</strong> returns both transcription and AI response</li>
<li><strong>File cleanup</strong> removes temporary audio file</li>
</ol>
<hr>
<h2 id="error-handling">Error Handling<a href="#error-handling" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="client-side-errors">Client-Side Errors<a href="#client-side-errors" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Microphone access</strong>: Graceful handling of permission denials</li>
<li><strong>Recording failures</strong>: User-friendly error messages</li>
<li><strong>Network issues</strong>: Timeout and connection error handling</li>
</ul>
<h3 id="server-side-errors">Server-Side Errors<a href="#server-side-errors" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>File validation</strong>: Check for audio file presence</li>
<li><strong>Transcription failures</strong>: Catch and report Whisper errors</li>
<li><strong>Cleanup assurance</strong>: Files removed even if processing fails</li>
</ul>
<h3 id="user-feedback">User Feedback<a href="#user-feedback" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="line"><span class="cl"><span class="kd">function</span> <span class="nx">showError</span><span class="p">(</span><span class="nx">message</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kr">const</span> <span class="nx">errorDiv</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">createElement</span><span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="nx">errorDiv</span><span class="p">.</span><span class="nx">style</span><span class="p">.</span><span class="nx">cssText</span> <span class="o">=</span> 
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;background-color: #f8d7da;&#39;</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;color: #721c24;&#39;</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;padding: 10px;&#39;</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;border-radius: 5px;&#39;</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;margin: 10px 0;&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="nx">errorDiv</span><span class="p">.</span><span class="nx">textContent</span> <span class="o">=</span> <span class="nx">message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// Auto-remove after 5 seconds
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nx">setTimeout</span><span class="p">(</span><span class="kd">function</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="nx">errorDiv</span><span class="p">.</span><span class="nx">parentNode</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nx">errorDiv</span><span class="p">.</span><span class="nx">parentNode</span><span class="p">.</span><span class="nx">removeChild</span><span class="p">(</span><span class="nx">errorDiv</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span> <span class="mi">5000</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><hr>
<h2 id="conclusion">Conclusion<a href="#conclusion" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>The Whisper integration successfully adds voice input capabilities to the AI assistant, providing a more natural and accessible user interface. The implementation demonstrates:</p>
<ul>
<li><strong>Modern web APIs</strong>: MediaRecorder for browser audio capture</li>
<li><strong>AI model integration</strong>: Whisper for accurate speech-to-text</li>
<li><strong>Robust error handling</strong>: Comprehensive error management</li>
<li><strong>User experience</strong>: Clear feedback and intuitive controls</li>
</ul>
<p>This feature significantly enhances the usability of the multi-tool assistant while maintaining the existing functionality and architecture patterns.</p>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h">Read other posts</span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
      <a href="//localhost:1313/posts/scalability_considerations/" class="button inline prev">
        &lt; [<span class="button__text">Scalability Considerations for Production Deployment</span>]
      </a>
    
    
      ::
    
    
      <a href="//localhost:1313/posts/available_tools/" class="button inline next">
         [<span class="button__text">Implemented Tools</span>] &gt;
      </a>
    
  </div>
</div>


  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
