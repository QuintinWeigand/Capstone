<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>Scalability Considerations for Production Deployment :: Terminal</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content=" Table of Contents Overview Current Architecture Limitations LLM Scaling Strategies MCP Server Scaling Whisper Model Scaling Database Scaling Container Orchestration Implementation Roadmap Overview While the current capstone project architecture works well for single-user development, scaling to production with multiple concurrent users requires strategic planning. This post explores scalability challenges and potential solutions for handling increased load while maintaining system reliability and performance.
Current Architecture Limitations Single-User Design One Ollama instance: Single Llama 3.1 8B model serving all requests Lazy Whisper loading: Model loads only on first voice request Single MCP server: One Go instance handling all tool calls No request queuing: All requests processed immediately or fail Bottlenecks Under Load LLM inference: Single model becomes throughput bottleneck Whisper cold starts: Each new user triggers model loading Memory constraints: 8GB per model limits concurrent instances Network latency: Single points of failure for all services LLM Scaling Strategies Horizontal Model Distribution Multiple Ollama Instances:
" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/scalability_considerations/" />





  
  <link rel="stylesheet" href="//localhost:1313/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">


<link rel="stylesheet" href="//localhost:1313/terminal.css">




<link rel="shortcut icon" href="//localhost:1313/favicon.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Scalability Considerations for Production Deployment">
<meta property="og:description" content=" Table of Contents Overview Current Architecture Limitations LLM Scaling Strategies MCP Server Scaling Whisper Model Scaling Database Scaling Container Orchestration Implementation Roadmap Overview While the current capstone project architecture works well for single-user development, scaling to production with multiple concurrent users requires strategic planning. This post explores scalability challenges and potential solutions for handling increased load while maintaining system reliability and performance.
Current Architecture Limitations Single-User Design One Ollama instance: Single Llama 3.1 8B model serving all requests Lazy Whisper loading: Model loads only on first voice request Single MCP server: One Go instance handling all tool calls No request queuing: All requests processed immediately or fail Bottlenecks Under Load LLM inference: Single model becomes throughput bottleneck Whisper cold starts: Each new user triggers model loading Memory constraints: 8GB per model limits concurrent instances Network latency: Single points of failure for all services LLM Scaling Strategies Horizontal Model Distribution Multiple Ollama Instances:
" />
<meta property="og:url" content="//localhost:1313/posts/scalability_considerations/" />
<meta property="og:site_name" content="Terminal" />

  <meta property="og:image" content="//localhost:1313/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2025-12-14 00:00:00 &#43;0000 UTC" />












</head>
<body>


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/showcase">Showcase</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="/showcase" >Showcase</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/scalability_considerations/">Scalability Considerations for Production Deployment</a>
  </h1>
  <div class="post-meta"><time class="post-date">2025-12-14</time></div>

  
  


  

  <div class="post-content"><div>
        <hr>
<h1 id="table-of-contents">Table of Contents<a href="#table-of-contents" class="hanchor" ariaLabel="Anchor">#</a> </h1>
<ol>
<li><a href="/posts/scalability_considerations/#overview">Overview</a></li>
<li><a href="/posts/scalability_considerations/#current_architecture_limitations">Current Architecture Limitations</a></li>
<li><a href="/posts/scalability_considerations/#llm_scaling_strategies">LLM Scaling Strategies</a></li>
<li><a href="/posts/scalability_considerations/#mcp_server_scaling">MCP Server Scaling</a></li>
<li><a href="/posts/scalability_considerations/#whisper_model_scaling">Whisper Model Scaling</a></li>
<li><a href="/posts/scalability_considerations/#database_scaling">Database Scaling</a></li>
<li><a href="/posts/scalability_considerations/#container_orchestration">Container Orchestration</a></li>
<li><a href="/posts/scalability_considerations/#implementation_roadmap">Implementation Roadmap</a></li>
</ol>
<hr>
<h2 id="overview">Overview<a href="#overview" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>While the current capstone project architecture works well for single-user development, scaling to production with multiple concurrent users requires strategic planning. This post explores scalability challenges and potential solutions for handling increased load while maintaining system reliability and performance.</p>
<hr>
<h2 id="current-architecture-limitations">Current Architecture Limitations<a href="#current-architecture-limitations" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="single-user-design">Single-User Design<a href="#single-user-design" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>One Ollama instance</strong>: Single Llama 3.1 8B model serving all requests</li>
<li><strong>Lazy Whisper loading</strong>: Model loads only on first voice request</li>
<li><strong>Single MCP server</strong>: One Go instance handling all tool calls</li>
<li><strong>No request queuing</strong>: All requests processed immediately or fail</li>
</ul>
<h3 id="bottlenecks-under-load">Bottlenecks Under Load<a href="#bottlenecks-under-load" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>LLM inference</strong>: Single model becomes throughput bottleneck</li>
<li><strong>Whisper cold starts</strong>: Each new user triggers model loading</li>
<li><strong>Memory constraints</strong>: 8GB per model limits concurrent instances</li>
<li><strong>Network latency</strong>: Single points of failure for all services</li>
</ul>
<hr>
<h2 id="llm-scaling-strategies">LLM Scaling Strategies<a href="#llm-scaling-strategies" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="horizontal-model-distribution">Horizontal Model Distribution<a href="#horizontal-model-distribution" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Multiple Ollama Instances:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="c"># Docker Compose example</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">services</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ollama-1</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">ollama/ollama</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">deploy</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">reservations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">8G</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">8G</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ollama-2</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">ollama/ollama</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">deploy</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">reservations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">8G</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">8G</span><span class="w">
</span></span></span></code></pre></div><p><strong>Load Balancing Approach:</strong>
For LLM scaling, implement load balancing to distribute requests across multiple Ollama instances. This could be done using Nginx or HAProxy to route incoming requests to available model servers, ensuring no single instance becomes a bottleneck.</p>
<h3 id="model-specific-routing">Model-Specific Routing<a href="#model-specific-routing" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Request Classification:</strong></p>
<ul>
<li><strong>Simple queries</strong>: Route to smaller, faster models (Gemma 2B)</li>
<li><strong>Complex tool calls</strong>: Route to larger models (Llama 3.1 8B)</li>
<li><strong>Voice processing</strong>: Route to models optimized for audio context</li>
<li><strong>Cost optimization</strong>: Use appropriate model size per task</li>
</ul>
<p><strong>Queue System Approach:</strong>
Implement a request queuing system to handle higher loads more effectively. Instead of processing all requests immediately, queue them and process based on available resources. This prevents system overload during peak usage and ensures fair request handling. Redis or RabbitMQ could serve as the queue backend, with workers pulling requests based on model availability and system capacity.</p>
<hr>
<h2 id="mcp-server-scaling">MCP Server Scaling<a href="#mcp-server-scaling" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="go-concurrency-advantages">Go Concurrency Advantages<a href="#go-concurrency-advantages" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Goroutine-Based Scaling:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="line"><span class="cl"><span class="kd">func</span><span class="w"> </span><span class="nf">handleToolRequest</span><span class="p">(</span><span class="nx">w</span><span class="w"> </span><span class="nx">http</span><span class="p">.</span><span class="nx">ResponseWriter</span><span class="p">,</span><span class="w"> </span><span class="nx">r</span><span class="w"> </span><span class="o">*</span><span class="nx">http</span><span class="p">.</span><span class="nx">Request</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="c1">// Process requests concurrently</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="k">go</span><span class="w"> </span><span class="kd">func</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nx">result</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nf">executeTool</span><span class="p">(</span><span class="nx">toolCall</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nx">responseChannel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nx">result</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="p">}()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="p">}</span><span class="w">
</span></span></span></code></pre></div><p><strong>Connection Pooling:</strong>
Implement database connection pooling to efficiently manage database connections. Instead of creating new connections for each request, maintain a pool of reusable connections. This reduces overhead and improves performance under load by eliminating connection setup time for frequent database operations.</p>
<h3 id="horizontal-scaling-strategy">Horizontal Scaling Strategy<a href="#horizontal-scaling-strategy" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Multiple MCP Instances:</strong></p>
<ul>
<li><strong>Load balancer</strong>: Nginx/HAProxy distributing requests</li>
<li><strong>Stateless design</strong>: Current architecture already supports multiple instances</li>
<li><strong>Health checks</strong>: Automatic removal of failed instances</li>
<li><strong>Session affinity</strong>: Not required due to stateless design</li>
</ul>
<p><strong>Database Replication:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="line"><span class="cl"><span class="c1">// MongoDB replica set configuration</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nx">clientOptions</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">options</span><span class="p">.</span><span class="nf">Client</span><span class="p">().</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nf">ApplyURI</span><span class="p">(</span><span class="s">&#34;mongodb://mongo1:27017,mongo2:27017,mongo3:27017/?replicaSet=rs0&#34;</span><span class="p">)</span><span class="w">
</span></span></span></code></pre></div><hr>
<h2 id="whisper-model-scaling">Whisper Model Scaling<a href="#whisper-model-scaling" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="current-lazy-loading-problem">Current Lazy Loading Problem<a href="#current-lazy-loading-problem" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Single Instance Bottleneck:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Current implementation - problematic under load</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">load_whisper_model</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">global</span> <span class="n">whisper_model</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">whisper_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">whisper_model</span> <span class="o">=</span> <span class="n">whisper</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;base&#34;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">whisper_model</span>
</span></span></code></pre></div><h3 id="scaling-solutions">Scaling Solutions<a href="#scaling-solutions" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Pre-Loading Strategy:</strong>
Instead of lazy loading the Whisper model only when needed, load it at application startup. This eliminates the cold start problem where first voice request experiences significant delay. The model stays in memory and is immediately available for transcription requests.</p>
<p><strong>Model Pooling:</strong>
For handling concurrent voice requests, maintain multiple instances of the Whisper model. When a transcription request comes in, assign it to an available model instance from the pool. This allows multiple users to transcribe audio simultaneously without waiting for model loading.</p>
<p><strong>Distributed Processing:</strong>
Consider separating transcription into its own microservice. Audio files could be uploaded and processed asynchronously, with results returned via polling or websockets. This prevents the main application from being blocked by transcription processing time.</p>
<hr>
<h2 id="database-scaling">Database Scaling<a href="#database-scaling" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="mongodb-scaling-strategies">MongoDB Scaling Strategies<a href="#mongodb-scaling-strategies" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Database Replication:</strong>
Implement MongoDB replica sets to ensure data availability and read scalability. Multiple database instances can handle read requests concurrently, with automatic failover if primary goes down. This provides both reliability and improved read performance.</p>
<p><strong>Sharding for Large Datasets:</strong>
For handling large amounts of user data, implement database sharding to distribute data across multiple servers. This allows horizontal scaling of database layer as user base grows.</p>
<p><strong>Caching Layer:</strong>
Add Redis caching layer for frequently accessed tool results. Common requests like weather data or recent weight history can be cached to reduce database load and improve response times.</p>
<hr>
<h2 id="container-orchestration">Container Orchestration<a href="#container-orchestration" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="kubernetes-deployment">Kubernetes Deployment<a href="#kubernetes-deployment" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><strong>Container Deployment:</strong>
Use Kubernetes or Docker Compose to orchestrate containerized services. Each component (Flask app, MCP server, Ollama instances) runs in separate containers with defined resource limits and automatic restart policies.</p>
<p><strong>Service Discovery:</strong>
Implement service discovery mechanisms so containers can find and communicate with each other. Kubernetes provides built-in service discovery, or use Docker networking with container names for inter-service communication.</p>
<p><strong>Auto-Scaling:</strong>
Configure horizontal pod autoscaling based on CPU and memory usage. When load increases, automatically add more instances of services, and scale down when demand decreases to optimize resource usage.</p>
<hr>
<h2 id="theoretical-implementation-roadmap">Theoretical Implementation Roadmap<a href="#theoretical-implementation-roadmap" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="phase-1-basic-scaling">Phase 1: Basic Scaling<a href="#phase-1-basic-scaling" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ol>
<li><strong>Containerize all services</strong>: Docker images for Flask, MCP, Ollama</li>
<li><strong>Implement load balancing</strong>: Nginx for request distribution</li>
<li><strong>Add health checks</strong>: Service monitoring and automatic restart</li>
<li><strong>Database replication</strong>: MongoDB replica set for high availability</li>
</ol>
<h3 id="phase-2-advanced-scaling">Phase 2: Advanced Scaling<a href="#phase-2-advanced-scaling" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ol>
<li><strong>Request queuing</strong>: Redis for LLM and Whisper request management</li>
<li><strong>Model pooling</strong>: Multiple Whisper instances for concurrent processing</li>
<li><strong>Caching layer</strong>: Redis for frequent tool results</li>
<li><strong>Auto-scaling</strong>: Kubernetes HPA based on CPU/memory usage</li>
</ol>
<hr>
<h2 id="conclusion">Conclusion<a href="#conclusion" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>Scaling the multi-tool AI assistant from single-user development to production requires addressing multiple bottlenecks simultaneously. The current architecture provides a solid foundation with its stateless design and clean service separation, but needs strategic enhancements for concurrent user support.</p>
<p>Key scaling priorities should be:</p>
<ol>
<li><strong>LLM horizontal scaling</strong> for request throughput</li>
<li><strong>Whisper pre-loading</strong> to eliminate cold starts</li>
<li><strong>Database replication</strong> for data reliability</li>
<li><strong>Container orchestration</strong> for automated management</li>
</ol>
<p>By implementing these strategies systematically, the system can scale from single-user development to handling hundreds of concurrent users while maintaining the reliability and functionality demonstrated in the base capstone project.</p>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h">Read other posts</span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
      <a href="//localhost:1313/posts/frontend_development/" class="button inline prev">
        &lt; [<span class="button__text">Frontend Development</span>]
      </a>
    
    
      ::
    
    
      <a href="//localhost:1313/posts/whisper_integration/" class="button inline next">
         [<span class="button__text">Whisper Integration for Voice Input</span>] &gt;
      </a>
    
  </div>
</div>


  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
